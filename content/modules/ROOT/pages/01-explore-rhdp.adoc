= Lab 01 - Explore OpenShift cluster

We will begin with a quick tour of the lab environment. This lab
consists of a multi-node openshift cluster which has been deployed
exclusively for you. 
This is a sandbox environment. Feel free to play around.  

If you prefer to use your own terminal, you can login to the bastion for the lab with ssh


`ssh {bastion_ssh_user_name}@{bastion_public_hostname}`


== Red Hat Openshift Container Platform configuration review

Let's start by getting the nodes in our current cluster.  Open the tab to access the bastion.

+
[source,bash]
----
oc get nodes
----

This cluster has 3 control-plane nodes and 3 worker nodes.

We can check the OpenShift version we are running with the
following command:

+
[source,bash]
----
oc version
----

We can check the status of the cluster by running:

+
[source,bash]
----
oc cluster-info
----

== Portworx configuration review

Portworx Enterprise is already installed and running on this cluster. We
will investigate the configuration in the next section:

What does Portworx need to be installed?

[arabic]
. *Drives*: The drive /dev/nvme1n1 is available on each node which we
will be using.
. *Key Value Database (KVDB)*: Such as ETCD. We will be using the
Portworx Built-in KVDB instead of deploying our own.
. *Specification*: Portworx is defined by a spec file, we will create
the Portworx cluster using the spec URL.

+
[source,bash]
----
oc get pods -o wide -n portworx -l name=portworx
----

Check the installation logs:

+
[source,bash]
----
oc -n portworx logs -f $(oc get pods -l name=portworx -n portworx -o jsonpath='{.items[0].metadata.name}')  -c portworx
----

== Add PXCTL alias

Portworx ships with a
https://docs.portworx.com/portworx-enterprise/reference/cli/pxctl-reference/status-reference[pxctl]
command line that that allows users to manage Portworx resources across the cluster.

To make it easier to run `pxctl1 commands, we will add an alias to the
shell.

+
[source,bash]
----
alias pxctl="oc -n portworx exec $(oc get pods -l name=portworx -n portworx -o jsonpath='{.items[0].metadata.name}') -c portworx -it -- /opt/pwx/bin/pxctl"
echo 'alias pxctl="oc -n portworx exec $(oc get pods -l name=portworx -n portworx -o jsonpath='{.items[0].metadata.name}') -c portworx -it -- /opt/pwx/bin/pxctl"' >> ~/.bashrc
----

Take a moment to review the portworx configuration by running the
following command:

+
[source,bash]
----
pxctl status
----

Notice that our Portworx cluster is running on 3 nodes.

== OpenShift Web Console

Next, let's take a quick look at the OpenShift Web console

The username is `{openshift_cluster_admin_username}` and the password is `{openshift_cluster_admin_password}`

Web Console URL

`{openshift_console_url}`


Once logged in, navigate to `Operators/Installed Operators`. Change your Project to `portwrox`.  You should see Portworx Etnerprise installed with a status of Succeeded.

image::01_01_operator.png[]

Click on the `Portworx Enterprise` operator and then Click `Stoage Cluster`

`portworx-storage-clusteer` should be in the `Running` phase

image::01_02_portworx_operator.png[]